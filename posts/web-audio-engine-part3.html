<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/19d11c52b217898d.css" crossorigin="" data-precedence="next"/><link rel="stylesheet" href="/_next/static/css/c95096fc0885ed51.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-84ba3017a29c8664.js" crossorigin=""/><script src="/_next/static/chunks/90234aad-94b9601bc92ba32c.js" async="" crossorigin=""></script><script src="/_next/static/chunks/672-712a286b3b50ffb9.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-cdb975597a421eb2.js" async="" crossorigin=""></script><script src="/_next/static/chunks/53dc2434-cbba9649805a64e5.js" async=""></script><script src="/_next/static/chunks/139-49b50a3bf6f25794.js" async=""></script><script src="/_next/static/chunks/254-700dd0f479925ac0.js" async=""></script><script src="/_next/static/chunks/app/posts/web-audio-engine-part3/page-160df2674662c7c1.js" async=""></script><script src="/_next/static/chunks/313-3c66015e67682b11.js" async=""></script><script src="/_next/static/chunks/app/posts/layout-3c2a5579c52700c6.js" async=""></script><script src="/_next/static/chunks/app/page-f4dd77ccded4be46.js" async=""></script><title>Web Audio Engine Part 3 - Integrating WebAudio API</title><meta name="description" content="It&#x27;t time to start intergrating WebAudio API to our project!"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__className_aaf875 flex min-h-screen flex-col"><header class="container flex items-center justify-between py-4 dark:bg-gray-800"><h1 class="text-xl font-semibold text-gray-900 dark:text-white"><a href="/">Mike Zaby</a></h1><nav><ul class="flex space-x-4"><li><a target="_blank" href="https://github.com/mikezaby" class="transition-colors hover:text-gray-600 dark:hover:text-white"><svg viewBox="0 0 16 16" class="h-5 w-5" fill="currentColor" aria-hidden="true"><path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path></svg></a></li><li><a target="_blank" href="https://twitter.com/mikezaby" class="transition-colors hover:text-blue-500"><svg viewBox="0 0 24 24" class="h-5 w-5" fill="currentColor" aria-hidden="true"><path d="M24 4.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.798-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.897-.957-2.178-1.555-3.594-1.555-3.179 0-5.515 2.966-4.797 6.045-4.091-.205-7.719-2.165-10.148-5.144-1.29 2.213-.669 5.108 1.523 6.574-.806-.026-1.566-.247-2.229-.616-.054 2.281 1.581 4.415 3.949 4.89-.693.188-1.452.232-2.224.084.626 1.956 2.444 3.379 4.6 3.419-2.07 1.623-4.678 2.348-7.29 2.04 2.179 1.397 4.768 2.212 7.548 2.212 9.142 0 14.307-7.721 13.995-14.646.962-.695 1.797-1.562 2.457-2.549z"></path></svg></a></li><li><a target="_blank" href="https://youtube.com/@mizakiro" class="transition-colors hover:text-red-600"><svg viewBox="0 0 24 24" class="h-5 w-5" fill="currentColor" aria-hidden="true"><path d="M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z"></path></svg></a></li><li><a target="_blank" href="https://www.linkedin.com/in/michalis-zabaras-97b5002b" class="transition-colors hover:text-blue-700"><svg viewBox="0 0 24 24" class="h-5 w-5" fill="currentColor" aria-hidden="true"><path d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path></svg></a></li><li><a class="font-bold transition-colors hover:text-gray-600 dark:hover:text-white" href="/about">About</a></li></ul></nav></header><div class="separator"></div><main class="container mt-8 flex-grow"><div><article class="markdown-body"><h1>Web Audio Engine Part 3 - Integrating WebAudio API</h1>
<p>In the <a href="/posts/web-audio-engine-part2">previous post</a>, we built the foundation of our Engine, and now we are ready to begin integrating the WebAudio API into our project.
More specifically, we will incorporate the WebAudio <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioContext">AudioContext</a>
and <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode">AudioNode</a>.</p>
<p>You can find the codebase up to this point on the <a href="https://github.com/mikezaby/web_audio_engine/tree/audio-node">audio-node</a> branch,
or you can view the additions compared with the previous post <a href="https://github.com/mikezaby/web_audio_engine/compare/basic-module...audio-node">here</a>.</p>
<h2>AudioContext</h2>
<p><code>AudioContext</code> is the backbone of the WebAudio API. To do anything with WebAudio, you must first create an AudioContext.
It manages almost everything in WebAudio:</p>
<ul>
<li><strong>Set the Sample Rate and Latency</strong>: Optimize audio processing to balance between quality (higher sample rates) and responsiveness (lower latency), crucial for real-time applications.</li>
<li><strong>Create AudioNodes</strong>: Generate various sources, and processing modules.</li>
<li><strong>Manage Connections Between AudioNodes</strong>: Construct sophisticated audio processing graphs by connecting nodes in chains or more complex structures for versatile audio manipulation.</li>
<li><strong>Control Audio Playback</strong>: Manage when audio plays, stops, and how it&#x27;s synchronized with other media or user interactions.</li>
<li><strong>Get the Destination Node</strong>: Connect nodes to the context&#x27;s destination, typically the speakers or audio output device, to produce sound.</li>
</ul>
<h3>Integrate</h3>
<p>To begin, we will install <a href="https://github.com/chrisguttandin/standardized-audio-context">standardized-audio-context</a>
as we want to have a reliable and consistent way of work in all supported browsers.</p>
<pre><code>pnpm install standardized-audio-context
</code></pre>
<p>Now, we will create <code>core/Context</code> that is responsible for creating or getting the current context.
We have two kinds of context, the <code>AudioContext</code> and the <code>OfflineAudioContext</code>.
The difference between that two, is that the first one is rendirng to the hardware output,
and the offline is rendiring to an <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer">AudioBuffer</a>.
So if we want to have a real time behavior we use the <code>AudioContext</code>, on the other hand if we want to generate a wav or mp3 we use the <code>OfflineAudioContext</code>.
We will not use <code>OfflineAudioContext</code>, but we prepare our codebase for this.</p>
<pre><code class="language-ts">// file: /src/core/Context.ts

import {
  AudioContext,
  IAudioContext,
  IOfflineAudioContext,
  OfflineAudioContext,
} from &quot;standardized-audio-context&quot;;

export type IAnyAudioContext = IAudioContext | IOfflineAudioContext;

let globalContext: IAnyAudioContext;

export function getContext(): IAnyAudioContext {
  if (globalContext) return globalContext;

  setNewAudioContext();

  return globalContext;
}

export function setNewAudioContext() {
  const context = new AudioContext();
  setContext(context);
}

interface OfflineAudioContextProps {
  length: number;
  sampleRate: number;
}

export function setNewOfflineAudioContext(props: OfflineAudioContextProps) {
  const context = new OfflineAudioContext(props);
  setContext(context);
}

function setContext(context: IAnyAudioContext) {
  globalContext = context;
}
</code></pre>
<p>Update Engine and Module to assign a context on initialization.</p>
<p>Add property:</p>
<pre><code class="language-ts">context: IAnyAudioContext;
</code></pre>
<p>Initialize in constructor:</p>
<pre><code class="language-ts">this.context = getContext();
</code></pre>
<h2>AudioNode</h2>
<p>As we have integrated <strong>AudioContext</strong>, which is a prerequisite for creating <strong>AudioNodes</strong>.</p>
<p>In the WebAudio API, an AudioNode is a fundamental component used to construct an audio processing graph.
AudioNodes are individual audio processing units that can generate, shape, manipulate, or analyze audio data.
Each AudioNode can be connected to other AudioNodes, creating a network where audio signals flow from one node to another.</p>
<p>There are several types of AudioNodes, each serving specific purposes:</p>
<ul>
<li><strong>Source Nodes:</strong> Generate audio signals, e.g., <code>OscillatorNode</code> for tones or <code>AudioBufferSourceNode</code> for playing audio samples.</li>
<li><strong>Processing Nodes:</strong> Modify audio signals, e.g., <code>GainNode</code> for volume control, <code>BiquadFilterNode</code> for tone shaping.</li>
<li><strong>Destination Nodes:</strong> The final node in the audio graph is <code>AudioDestinationNode</code> that outputs the audio to the system&#x27;s audio output device.</li>
</ul>
<h3>Integrate</h3>
<p>We want the actual modules to pass the corresponding AudioNode to the abstract Module class.
However, since we need the AudioContext to create an AudioNode, and we don&#x27;t have access to <code>this.context</code> before calling <code>super()</code>, we pass a callback to super that obtains the context and returns the AudioNode instance.</p>
<p>We define an interface for our Module constructor.</p>
<pre><code class="language-ts">// file: /src/core/Module.ts
interface IModuleConstructor&lt;T extends ModuleType&gt;
  extends Optional&lt;IModule&lt;T&gt;, &quot;id&quot;&gt; {
  audioNode: (context: IAnyAudioContext) =&gt; IAudioNode&lt;IAnyAudioContext&gt;;
}
</code></pre>
<pre><code class="language-ts">constructor(params: IModuleConstructor&lt;T&gt;) {
</code></pre>
<p>In the constructor we assign AudioNode</p>
<pre><code class="language-ts">this.audioNode = audioNode(this.context);
</code></pre>
<p>In the Oscillator module, we define the AudioNode callback and pass it to super.</p>
<pre><code class="language-ts">// file: /src/modules/Oscillator.ts
import { OscillatorNode } from &quot;standardized-audio-context&quot;;
import { IChildParams, ModuleType } from &quot;.&quot;;
import { IAnyAudioContext } from &quot;../core&quot;;
import Module, { IModule } from &quot;../core/Module&quot;;

export interface IOscillator extends IModule&lt;ModuleType.Oscillator&gt; {}

export interface IOscillatorProps {
  wave: OscillatorType;
  frequency: number;
}

const DEFAULT_PROPS: IOscillatorProps = { wave: &quot;sine&quot;, frequency: 440 };

export default class Oscillator extends Module&lt;ModuleType.Oscillator&gt; {
  declare audioNode: OscillatorNode&lt;IAnyAudioContext&gt;;

  constructor(params: IChildParams&lt;ModuleType.Oscillator&gt;) {
    const props = { ...DEFAULT_PROPS, ...params.props };
    const audioNode = (context: IAnyAudioContext) =&gt;
      new OscillatorNode(context);

    super({ ...params, props, audioNode, moduleType: ModuleType.Oscillator });
  }
}
</code></pre>
<h2>AudioParam</h2>
<p>An <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioParam">AudioParam</a> is an interface representing an audio-related parameter, typically a volume, frequency, or similar property.
AudioParams are used to automate and control changes to AudioNode parameters over time.</p>
<p>For example, an OscillatorNode has a frequency AudioParam that can be set or modulated to change the pitch of the tone it generates.
This modulation can be done statically or over a period, allowing for sophisticated audio effects like sweeps and transitions.</p>
<h3>Integrate</h3>
<p>We need a mechanism that triggers a function when props are updated, enabling us to integrate with the <strong>AudioNode</strong> or more specifically, with the <strong>AudioParam</strong>.</p>
<p>For this reason, we will use <code>Object.assign</code> to trigger the setter of those props:</p>
<pre><code class="language-ts">// file: /src/core/Module.ts
set props(value: Partial&lt;ModuleTypeToPropsMapping[T]&gt;) {
  this._props = { ...this._props, ...value };
  Object.assign(this, value);
}
</code></pre>
<p>And this is how we interact with <strong>AudioNode</strong> and <strong>AudioParam</strong> in Oscillator</p>
<pre><code class="language-ts">// file: /src/modules/Oscillator.ts
set wave(value: IOscillatorProps[&quot;wave&quot;]) {
  this.audioNode.type = value;
}

set frequency(value: IOscillatorProps[&quot;frequency&quot;]) {
  this.audioNode.frequency.value = value;
}
</code></pre>
<p>We apply similar changes to the <a href="https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/modules/Volume.ts">Volume</a> module.</p>
<h2>Connect modules</h2>
<p>From <a href="https://developer.mozilla.org/en-US/docs/Web/API/AudioNode/connect">MDN</a></p>
<blockquote>
<p>The <code>connect()</code> method of the AudioNode interface lets you connect one of the node&#x27;s outputs to a target, which may be either another <strong>AudioNode</strong> (thereby directing the sound data to the specified node) or an <strong>AudioParam</strong>, so that the node&#x27;s output data is automatically used to change the value of that parameter over time.</p>
</blockquote>
<p>We&#x27;ll provide a temporary solution about how to connect modules, as we plan to implement a more advanced version in the next post.</p>
<p>We implement a connect function in the Module class, which takes another Module as an argument and then connects the AudioNodes of both.</p>
<pre><code class="language-ts">// file: /src/core/Module.ts
connect(module: AnyModule) {
  this.audioNode.connect(module.audioNode);
}
</code></pre>
<p>Then we expose this method to <strong>Engine</strong></p>
<pre><code class="language-ts">// file: /src/Engine.ts
connect(outputModuleId: string, inputModuleId: string) {
  const output = this.findModule(outputModuleId);
  const input = this.findModule(inputModuleId);

  output.connect(input);
}
</code></pre>
<h2>Startable</h2>
<p>There are <strong>AudioNodes</strong> that generate signals and have the ability to <strong>start</strong> and <strong>stop</strong>, such as the <code>OscillatorNode</code>.
We need to provide the ability to start and stop at the module level.</p>
<p>We implement an interface that describes the <strong>start</strong> and <strong>stop</strong> functions, and then each module is responsible for defining how to handle this.</p>
<pre><code class="language-ts">// file: /src/core/Module.ts
export interface Startable {
  start(time: number): void;
  stop(time: number): void;
}
</code></pre>
<p>The <code>Oscillator</code> implements this inteface:</p>
<pre><code class="language-ts">class Oscillator
  extends Module&lt;ModuleType.Oscillator&gt;
  implements IOscillatorProps, Startable {
</code></pre>
<p>And defines how this functions work:</p>
<pre><code class="language-ts">start(time: number) {
  this.audioNode.start(time);
}

stop(time: number) {
  this.audioNode.stop(time);

  this.audioNode = new OscillatorNode(this.context, {
    type: this.props[&quot;wave&quot;],
    frequency: this.props[&quot;frequency&quot;],
  });
}
</code></pre>
<p>Since an <code>OscillatorNode</code> can start only once, the only way to provide this functionality is to assign a new OscillatorNode to the audioNode property after the module stops.</p>
<p>We also need to expose <strong>start</strong> and <strong>stop</strong> functionality to the Engine:</p>
<pre><code class="language-ts">// file: /src/Engine.ts
async start(time?: number) {
  await this.resume();

  time ??= this.context.currentTime;
  this.isStarted = true;

  Object.values(this.modules).forEach((m) =&gt; {
    const module = m as unknown as Startable;
    if (!module.start) return;

    module.start(time);
  });
}

stop(time?: number) {
  time ??= this.context.currentTime;
  this.isStarted = false;

  Object.values(this.modules).forEach((m) =&gt; {
    const module = m as unknown as Startable;
    if (!module.stop) return;

    module.stop(time);
  });
}

async resume() {
  if (this.context instanceof OfflineAudioContext) return;

  return await this.context.resume();
}
</code></pre>
<p>We also need to <code>resume</code> the <code>AudioContext</code>, as the browser starts the context in a <code>suspended</code> state.</p>
<h2>Master</h2>
<p>We have already implemented the Oscillator and Volume modules, but to drive our signal to the computer&#x27;s output, we need at least one more module.
For this purpose, we will implement the <a href="https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/modules/Master.ts">Master module</a>.
This module utilizes the AudioDestinationNode as its AudioNode, which serves as the final destination for all audio signals in our audio processing graph.
This is where audio gets routed to the hardware, such as speakers or headphones, enabling us to hear the sound.</p>
<h2>Time for action</h2>
<p>Let&#x27;s assume that we have the data for an Oscillator, Volume, and Master assigned to the variables osc, vol, and master, respectively.
We want to create a routing path like this:</p>
<pre><code>osc -&gt; vol -&gt; master
</code></pre>
<pre><code class="language-ts">Engine.connect(osc.id, vol.id);
Engine.connect(vol.id, master.id);
</code></pre>
<p>We create a function to toggle (start/stop) the <strong>Engine</strong>:</p>
<pre><code class="language-ts">async function toggle() =&gt; {
  if (Engine.isStarted) {
    Engine.stop();
    // This is temporary, we will implement a automate solution for this
    Engine.connect(osc.id, vol.id);
  } else {
    await Engine.start();
  }
};
</code></pre>
<p>To make things more interesting, we define an interval that updates the oscillator&#x27;s frequency over time:</p>
<pre><code class="language-ts">setInterval(() =&gt; {
  Engine.updateModule({
    id: osc.id,
    moduleType: osc.moduleType,
    changes: { props: { frequency: 2000 * Math.random() } },
  });
}, 1000);
</code></pre>
<p>Let&#x27;s hear the result:</p>
<!-- -->
<p>The complete example of this implementation is available <a href="https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/main.ts">here</a></p>
<h2>What&#x27;s Next</h2>
<p>In the next post, we will implement more advanced I/O handling, which will provide information about the available inputs, outputs, and current routing configurations.</p></article><div id="disqus_thread"></div></div></main><footer class="container py-8 text-center"><p class="text-gray-600 dark:text-gray-300">© 2024 Michalis Zabaras. All rights reserved.</p></footer><script src="/_next/static/chunks/webpack-84ba3017a29c8664.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/c9a5bc6a7c948fb0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/19d11c52b217898d.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:HL[\"/_next/static/css/c95096fc0885ed51.css\",\"style\",{\"crossOrigin\":\"\"}]\n"])</script><script>self.__next_f.push([1,"5:I[7732,[],\"\"]\n7:I[3862,[\"221\",\"static/chunks/53dc2434-cbba9649805a64e5.js\",\"139\",\"static/chunks/139-49b50a3bf6f25794.js\",\"254\",\"static/chunks/254-700dd0f479925ac0.js\",\"784\",\"static/chunks/app/posts/web-audio-engine-part3/page-160df2674662c7c1.js\"],\"\"]\n8:I[3107,[],\"\"]\n9:I[6865,[],\"\"]\na:I[2088,[\"139\",\"static/chunks/139-49b50a3bf6f25794.js\",\"254\",\"static/chunks/254-700dd0f479925ac0.js\",\"313\",\"static/chunks/313-3c66015e67682b11.js\",\"517\",\"static/chunks/app/posts/layout-3c2a5579c52700c6.js\"],\"\"]\nb:I[9254,[\"221\",\"static/chunks/53dc2434-cbba9649805a64e5.js\",\"139\",\"static/chunks/139-49b50a3bf6f25794.js\",\"254\",\"static/chunks/254-700dd0f479925ac0.js\",\"931\",\"static/chunks/app/page-f4dd77ccded4be46.js\"],\"\"]\nd:I[8022,[],\"\"]\ne:[]\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/19d11c52b217898d.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L5\",null,{\"buildId\":\"TRuVdNvaUg1Y9AA4eH7uh\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/posts/web-audio-engine-part3\",\"initialTree\":[\"\",{\"children\":[\"posts\",{\"children\":[\"web-audio-engine-part3\",{\"children\":[\"__PAGE__\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"posts\",{\"children\":[\"web-audio-engine-part3\",{\"children\":[\"__PAGE__\",{},[\"$L6\",[[\"$\",\"h1\",null,{\"children\":\"Web Audio Engine Part 3 - Integrating WebAudio API\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"In the \",[\"$\",\"a\",null,{\"href\":\"/posts/web-audio-engine-part2\",\"children\":\"previous post\"}],\", we built the foundation of our Engine, and now we are ready to begin integrating the WebAudio API into our project.\\nMore specifically, we will incorporate the WebAudio \",[\"$\",\"a\",null,{\"href\":\"https://developer.mozilla.org/en-US/docs/Web/API/AudioContext\",\"children\":\"AudioContext\"}],\"\\nand \",[\"$\",\"a\",null,{\"href\":\"https://developer.mozilla.org/en-US/docs/Web/API/AudioNode\",\"children\":\"AudioNode\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"You can find the codebase up to this point on the \",[\"$\",\"a\",null,{\"href\":\"https://github.com/mikezaby/web_audio_engine/tree/audio-node\",\"children\":\"audio-node\"}],\" branch,\\nor you can view the additions compared with the previous post \",[\"$\",\"a\",null,{\"href\":\"https://github.com/mikezaby/web_audio_engine/compare/basic-module...audio-node\",\"children\":\"here\"}],\".\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"AudioContext\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[[\"$\",\"code\",null,{\"children\":\"AudioContext\"}],\" is the backbone of the WebAudio API. To do anything with WebAudio, you must first create an AudioContext.\\nIt manages almost everything in WebAudio:\"]}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Set the Sample Rate and Latency\"}],\": Optimize audio processing to balance between quality (higher sample rates) and responsiveness (lower latency), crucial for real-time applications.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Create AudioNodes\"}],\": Generate various sources, and processing modules.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Manage Connections Between AudioNodes\"}],\": Construct sophisticated audio processing graphs by connecting nodes in chains or more complex structures for versatile audio manipulation.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Control Audio Playback\"}],\": Manage when audio plays, stops, and how it's synchronized with other media or user interactions.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Get the Destination Node\"}],\": Connect nodes to the context's destination, typically the speakers or audio output device, to produce sound.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Integrate\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"To begin, we will install \",[\"$\",\"a\",null,{\"href\":\"https://github.com/chrisguttandin/standardized-audio-context\",\"children\":\"standardized-audio-context\"}],\"\\nas we want to have a reliable and consistent way of work in all supported browsers.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"pnpm install standardized-audio-context\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Now, we will create \",[\"$\",\"code\",null,{\"children\":\"core/Context\"}],\" that is responsible for creating or getting the current context.\\nWe have two kinds of context, the \",[\"$\",\"code\",null,{\"children\":\"AudioContext\"}],\" and the \",[\"$\",\"code\",null,{\"children\":\"OfflineAudioContext\"}],\".\\nThe difference between that two, is that the first one is rendirng to the hardware output,\\nand the offline is rendiring to an \",[\"$\",\"a\",null,{\"href\":\"https://developer.mozilla.org/en-US/docs/Web/API/AudioBuffer\",\"children\":\"AudioBuffer\"}],\".\\nSo if we want to have a real time behavior we use the \",[\"$\",\"code\",null,{\"children\":\"AudioContext\"}],\", on the other hand if we want to generate a wav or mp3 we use the \",[\"$\",\"code\",null,{\"children\":\"OfflineAudioContext\"}],\".\\nWe will not use \",[\"$\",\"code\",null,{\"children\":\"OfflineAudioContext\"}],\", but we prepare our codebase for this.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/core/Context.ts\\n\\nimport {\\n  AudioContext,\\n  IAudioContext,\\n  IOfflineAudioContext,\\n  OfflineAudioContext,\\n} from \\\"standardized-audio-context\\\";\\n\\nexport type IAnyAudioContext = IAudioContext | IOfflineAudioContext;\\n\\nlet globalContext: IAnyAudioContext;\\n\\nexport function getContext(): IAnyAudioContext {\\n  if (globalContext) return globalContext;\\n\\n  setNewAudioContext();\\n\\n  return globalContext;\\n}\\n\\nexport function setNewAudioContext() {\\n  const context = new AudioContext();\\n  setContext(context);\\n}\\n\\ninterface OfflineAudioContextProps {\\n  length: number;\\n  sampleRate: number;\\n}\\n\\nexport function setNewOfflineAudioContext(props: OfflineAudioContextProps) {\\n  const context = new OfflineAudioContext(props);\\n  setContext(context);\\n}\\n\\nfunction setContext(context: IAnyAudioContext) {\\n  globalContext = context;\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Update Engine and Module to assign a context on initialization.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Add property:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"context: IAnyAudioContext;\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Initialize in constructor:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"this.context = getContext();\\n\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"AudioNode\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"As we have integrated \",[\"$\",\"strong\",null,{\"children\":\"AudioContext\"}],\", which is a prerequisite for creating \",[\"$\",\"strong\",null,{\"children\":\"AudioNodes\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the WebAudio API, an AudioNode is a fundamental component used to construct an audio processing graph.\\nAudioNodes are individual audio processing units that can generate, shape, manipulate, or analyze audio data.\\nEach AudioNode can be connected to other AudioNodes, creating a network where audio signals flow from one node to another.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"There are several types of AudioNodes, each serving specific purposes:\"}],\"\\n\",[\"$\",\"ul\",null,{\"children\":[\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Source Nodes:\"}],\" Generate audio signals, e.g., \",[\"$\",\"code\",null,{\"children\":\"OscillatorNode\"}],\" for tones or \",[\"$\",\"code\",null,{\"children\":\"AudioBufferSourceNode\"}],\" for playing audio samples.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Processing Nodes:\"}],\" Modify audio signals, e.g., \",[\"$\",\"code\",null,{\"children\":\"GainNode\"}],\" for volume control, \",[\"$\",\"code\",null,{\"children\":\"BiquadFilterNode\"}],\" for tone shaping.\"]}],\"\\n\",[\"$\",\"li\",null,{\"children\":[[\"$\",\"strong\",null,{\"children\":\"Destination Nodes:\"}],\" The final node in the audio graph is \",[\"$\",\"code\",null,{\"children\":\"AudioDestinationNode\"}],\" that outputs the audio to the system's audio output device.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Integrate\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We want the actual modules to pass the corresponding AudioNode to the abstract Module class.\\nHowever, since we need the AudioContext to create an AudioNode, and we don't have access to \",[\"$\",\"code\",null,{\"children\":\"this.context\"}],\" before calling \",[\"$\",\"code\",null,{\"children\":\"super()\"}],\", we pass a callback to super that obtains the context and returns the AudioNode instance.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We define an interface for our Module constructor.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/core/Module.ts\\ninterface IModuleConstructor\u003cT extends ModuleType\u003e\\n  extends Optional\u003cIModule\u003cT\u003e, \\\"id\\\"\u003e {\\n  audioNode: (context: IAnyAudioContext) =\u003e IAudioNode\u003cIAnyAudioContext\u003e;\\n}\\n\"}]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"constructor(params: IModuleConstructor\u003cT\u003e) {\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the constructor we assign AudioNode\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"this.audioNode = audioNode(this.context);\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the Oscillator module, we define the AudioNode callback and pass it to super.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/modules/Oscillator.ts\\nimport { OscillatorNode } from \\\"standardized-audio-context\\\";\\nimport { IChildParams, ModuleType } from \\\".\\\";\\nimport { IAnyAudioContext } from \\\"../core\\\";\\nimport Module, { IModule } from \\\"../core/Module\\\";\\n\\nexport interface IOscillator extends IModule\u003cModuleType.Oscillator\u003e {}\\n\\nexport interface IOscillatorProps {\\n  wave: OscillatorType;\\n  frequency: number;\\n}\\n\\nconst DEFAULT_PROPS: IOscillatorProps = { wave: \\\"sine\\\", frequency: 440 };\\n\\nexport default class Oscillator extends Module\u003cModuleType.Oscillator\u003e {\\n  declare audioNode: OscillatorNode\u003cIAnyAudioContext\u003e;\\n\\n  constructor(params: IChildParams\u003cModuleType.Oscillator\u003e) {\\n    const props = { ...DEFAULT_PROPS, ...params.props };\\n    const audioNode = (context: IAnyAudioContext) =\u003e\\n      new OscillatorNode(context);\\n\\n    super({ ...params, props, audioNode, moduleType: ModuleType.Oscillator });\\n  }\\n}\\n\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"AudioParam\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"An \",[\"$\",\"a\",null,{\"href\":\"https://developer.mozilla.org/en-US/docs/Web/API/AudioParam\",\"children\":\"AudioParam\"}],\" is an interface representing an audio-related parameter, typically a volume, frequency, or similar property.\\nAudioParams are used to automate and control changes to AudioNode parameters over time.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"For example, an OscillatorNode has a frequency AudioParam that can be set or modulated to change the pitch of the tone it generates.\\nThis modulation can be done statically or over a period, allowing for sophisticated audio effects like sweeps and transitions.\"}],\"\\n\",[\"$\",\"h3\",null,{\"children\":\"Integrate\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We need a mechanism that triggers a function when props are updated, enabling us to integrate with the \",[\"$\",\"strong\",null,{\"children\":\"AudioNode\"}],\" or more specifically, with the \",[\"$\",\"strong\",null,{\"children\":\"AudioParam\"}],\".\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"For this reason, we will use \",[\"$\",\"code\",null,{\"children\":\"Object.assign\"}],\" to trigger the setter of those props:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/core/Module.ts\\nset props(value: Partial\u003cModuleTypeToPropsMapping[T]\u003e) {\\n  this._props = { ...this._props, ...value };\\n  Object.assign(this, value);\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"And this is how we interact with \",[\"$\",\"strong\",null,{\"children\":\"AudioNode\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"AudioParam\"}],\" in Oscillator\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/modules/Oscillator.ts\\nset wave(value: IOscillatorProps[\\\"wave\\\"]) {\\n  this.audioNode.type = value;\\n}\\n\\nset frequency(value: IOscillatorProps[\\\"frequency\\\"]) {\\n  this.audioNode.frequency.value = value;\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We apply similar changes to the \",[\"$\",\"a\",null,{\"href\":\"https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/modules/Volume.ts\",\"children\":\"Volume\"}],\" module.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Connect modules\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"From \",[\"$\",\"a\",null,{\"href\":\"https://developer.mozilla.org/en-US/docs/Web/API/AudioNode/connect\",\"children\":\"MDN\"}]]}],\"\\n\",[\"$\",\"blockquote\",null,{\"children\":[\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"connect()\"}],\" method of the AudioNode interface lets you connect one of the node's outputs to a target, which may be either another \",[\"$\",\"strong\",null,{\"children\":\"AudioNode\"}],\" (thereby directing the sound data to the specified node) or an \",[\"$\",\"strong\",null,{\"children\":\"AudioParam\"}],\", so that the node's output data is automatically used to change the value of that parameter over time.\"]}],\"\\n\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We'll provide a temporary solution about how to connect modules, as we plan to implement a more advanced version in the next post.\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"We implement a connect function in the Module class, which takes another Module as an argument and then connects the AudioNodes of both.\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/core/Module.ts\\nconnect(module: AnyModule) {\\n  this.audioNode.connect(module.audioNode);\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Then we expose this method to \",[\"$\",\"strong\",null,{\"children\":\"Engine\"}]]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/Engine.ts\\nconnect(outputModuleId: string, inputModuleId: string) {\\n  const output = this.findModule(outputModuleId);\\n  const input = this.findModule(inputModuleId);\\n\\n  output.connect(input);\\n}\\n\"}]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Startable\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"There are \",[\"$\",\"strong\",null,{\"children\":\"AudioNodes\"}],\" that generate signals and have the ability to \",[\"$\",\"strong\",null,{\"children\":\"start\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"stop\"}],\", such as the \",[\"$\",\"code\",null,{\"children\":\"OscillatorNode\"}],\".\\nWe need to provide the ability to start and stop at the module level.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We implement an interface that describes the \",[\"$\",\"strong\",null,{\"children\":\"start\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"stop\"}],\" functions, and then each module is responsible for defining how to handle this.\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/core/Module.ts\\nexport interface Startable {\\n  start(time: number): void;\\n  stop(time: number): void;\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The \",[\"$\",\"code\",null,{\"children\":\"Oscillator\"}],\" implements this inteface:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"class Oscillator\\n  extends Module\u003cModuleType.Oscillator\u003e\\n  implements IOscillatorProps, Startable {\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"And defines how this functions work:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"start(time: number) {\\n  this.audioNode.start(time);\\n}\\n\\nstop(time: number) {\\n  this.audioNode.stop(time);\\n\\n  this.audioNode = new OscillatorNode(this.context, {\\n    type: this.props[\\\"wave\\\"],\\n    frequency: this.props[\\\"frequency\\\"],\\n  });\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"Since an \",[\"$\",\"code\",null,{\"children\":\"OscillatorNode\"}],\" can start only once, the only way to provide this functionality is to assign a new OscillatorNode to the audioNode property after the module stops.\"]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We also need to expose \",[\"$\",\"strong\",null,{\"children\":\"start\"}],\" and \",[\"$\",\"strong\",null,{\"children\":\"stop\"}],\" functionality to the Engine:\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"// file: /src/Engine.ts\\nasync start(time?: number) {\\n  await this.resume();\\n\\n  time ??= this.context.currentTime;\\n  this.isStarted = true;\\n\\n  Object.values(this.modules).forEach((m) =\u003e {\\n    const module = m as unknown as Startable;\\n    if (!module.start) return;\\n\\n    module.start(time);\\n  });\\n}\\n\\nstop(time?: number) {\\n  time ??= this.context.currentTime;\\n  this.isStarted = false;\\n\\n  Object.values(this.modules).forEach((m) =\u003e {\\n    const module = m as unknown as Startable;\\n    if (!module.stop) return;\\n\\n    module.stop(time);\\n  });\\n}\\n\\nasync resume() {\\n  if (this.context instanceof OfflineAudioContext) return;\\n\\n  return await this.context.resume();\\n}\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We also need to \",[\"$\",\"code\",null,{\"children\":\"resume\"}],\" the \",[\"$\",\"code\",null,{\"children\":\"AudioContext\"}],\", as the browser starts the context in a \",[\"$\",\"code\",null,{\"children\":\"suspended\"}],\" state.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Master\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We have already implemented the Oscillator and Volume modules, but to drive our signal to the computer's output, we need at least one more module.\\nFor this purpose, we will implement the \",[\"$\",\"a\",null,{\"href\":\"https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/modules/Master.ts\",\"children\":\"Master module\"}],\".\\nThis module utilizes the AudioDestinationNode as its AudioNode, which serves as the final destination for all audio signals in our audio processing graph.\\nThis is where audio gets routed to the hardware, such as speakers or headphones, enabling us to hear the sound.\"]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"Time for action\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Let's assume that we have the data for an Oscillator, Volume, and Master assigned to the variables osc, vol, and master, respectively.\\nWe want to create a routing path like this:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"children\":\"osc -\u003e vol -\u003e master\\n\"}]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"Engine.connect(osc.id, vol.id);\\nEngine.connect(vol.id, master.id);\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"We create a function to toggle (start/stop) the \",[\"$\",\"strong\",null,{\"children\":\"Engine\"}],\":\"]}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"async function toggle() =\u003e {\\n  if (Engine.isStarted) {\\n    Engine.stop();\\n    // This is temporary, we will implement a automate solution for this\\n    Engine.connect(osc.id, vol.id);\\n  } else {\\n    await Engine.start();\\n  }\\n};\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"To make things more interesting, we define an interval that updates the oscillator's frequency over time:\"}],\"\\n\",[\"$\",\"pre\",null,{\"children\":[\"$\",\"code\",null,{\"className\":\"language-ts\",\"children\":\"setInterval(() =\u003e {\\n  Engine.updateModule({\\n    id: osc.id,\\n    moduleType: osc.moduleType,\\n    changes: { props: { frequency: 2000 * Math.random() } },\\n  });\\n}, 1000);\\n\"}]}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"Let's hear the result:\"}],\"\\n\",[\"$\",\"$L7\",null,{}],\"\\n\",[\"$\",\"p\",null,{\"children\":[\"The complete example of this implementation is available \",[\"$\",\"a\",null,{\"href\":\"https://github.com/mikezaby/web_audio_engine/blob/audio-node/src/main.ts\",\"children\":\"here\"}]]}],\"\\n\",[\"$\",\"h2\",null,{\"children\":\"What's Next\"}],\"\\n\",[\"$\",\"p\",null,{\"children\":\"In the next post, we will implement more advanced I/O handling, which will provide information about the available inputs, outputs, and current routing configurations.\"}]],null]]},[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\",\"web-audio-engine-part3\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}]]},[null,[\"$\",\"$La\",null,{\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"posts\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"styles\":null}],\"params\":{}}],null]]},[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_aaf875 flex min-h-screen flex-col\",\"children\":[[\"$\",\"header\",null,{\"className\":\"container flex items-center justify-between py-4 dark:bg-gray-800\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-xl font-semibold text-gray-900 dark:text-white\",\"children\":[\"$\",\"$Lb\",null,{\"href\":\"/\",\"children\":\"Mike Zaby\"}]}],[\"$\",\"nav\",null,{\"children\":[\"$\",\"ul\",null,{\"className\":\"flex space-x-4\",\"children\":[[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://github.com/mikezaby\",\"className\":\"transition-colors hover:text-gray-600 dark:hover:text-white\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 16 16\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"path\",null,{\"d\":\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z\"}]}]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://twitter.com/mikezaby\",\"className\":\"transition-colors hover:text-blue-500\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"path\",null,{\"d\":\"M24 4.557c-.883.392-1.832.656-2.828.775 1.017-.609 1.798-1.574 2.165-2.724-.951.564-2.005.974-3.127 1.195-.897-.957-2.178-1.555-3.594-1.555-3.179 0-5.515 2.966-4.797 6.045-4.091-.205-7.719-2.165-10.148-5.144-1.29 2.213-.669 5.108 1.523 6.574-.806-.026-1.566-.247-2.229-.616-.054 2.281 1.581 4.415 3.949 4.89-.693.188-1.452.232-2.224.084.626 1.956 2.444 3.379 4.6 3.419-2.07 1.623-4.678 2.348-7.29 2.04 2.179 1.397 4.768 2.212 7.548 2.212 9.142 0 14.307-7.721 13.995-14.646.962-.695 1.797-1.562 2.457-2.549z\"}]}]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://youtube.com/@mizakiro\",\"className\":\"transition-colors hover:text-red-600\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"path\",null,{\"d\":\"M19.615 3.184c-3.604-.246-11.631-.245-15.23 0-3.897.266-4.356 2.62-4.385 8.816.029 6.185.484 8.549 4.385 8.816 3.6.245 11.626.246 15.23 0 3.897-.266 4.356-2.62 4.385-8.816-.029-6.185-.484-8.549-4.385-8.816zm-10.615 12.816v-8l8 3.993-8 4.007z\"}]}]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"a\",null,{\"target\":\"_blank\",\"href\":\"https://www.linkedin.com/in/michalis-zabaras-97b5002b\",\"className\":\"transition-colors hover:text-blue-700\",\"children\":[\"$\",\"svg\",null,{\"viewBox\":\"0 0 24 24\",\"className\":\"h-5 w-5\",\"fill\":\"currentColor\",\"aria-hidden\":\"true\",\"children\":[\"$\",\"path\",null,{\"d\":\"M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z\"}]}]}]}],[\"$\",\"li\",null,{\"children\":[\"$\",\"$Lb\",null,{\"href\":\"/about\",\"className\":\"font-bold transition-colors hover:text-gray-600 dark:hover:text-white\",\"children\":\"About\"}]}]]}]}]]}],[\"$\",\"div\",null,{\"className\":\"separator\"}],[\"$\",\"main\",null,{\"className\":\"container mt-8 flex-grow\",\"children\":[\"$\",\"$L8\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L9\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"styles\":[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/c95096fc0885ed51.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]]}]}],[\"$\",\"footer\",null,{\"className\":\"container py-8 text-center\",\"children\":[\"$\",\"p\",null,{\"className\":\"text-gray-600 dark:text-gray-300\",\"children\":\"© 2024 Michalis Zabaras. All rights reserved.\"}]}]]}]}],null]],\"initialHead\":[false,\"$Lc\"],\"globalErrorComponent\":\"$d\",\"missingSlots\":\"$We\"}]]\n"])</script><script>self.__next_f.push([1,"c:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Web Audio Engine Part 3 - Integrating WebAudio API\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"It't time to start intergrating WebAudio API to our project!\"}],[\"$\",\"meta\",\"4\",{\"name\":\"next-size-adjust\"}]]\n6:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>