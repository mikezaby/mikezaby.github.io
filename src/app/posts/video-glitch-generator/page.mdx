import CoverImg from "./cover.png";

export const metadata = {
  title: "Vibe Code Glitch Generator",
  description:
    "My first fully vibe-coded project â€” a video glitch generator that reacts to music, made completely through AI prompts.",
  date: "Oct 12, 2025",
  cover: CoverImg,
  openGraph: {
    title: "Vibe Code Glitch Generator",
    description:
      "My first fully vibe-coded project â€” a video glitch generator that reacts to music, made completely through AI prompts.",
    images: [
      {
        url: CoverImg.src,
        width: CoverImg.width,
        height: CoverImg.height,
        alt: "Video Glitch Generator Cover",
      },
    ],
  },
};

# Vibe Code Glitch Generator

Iâ€™ve been a web developer for almost 15 years, and I love coding â€” with or without help from AI.
But this is the first time Iâ€™ve done something completely vibe coded â€” without editing even a single line by hand â€” and this is the story.

I wanted to upload a music track I had produced in the past.
At first, I thought Iâ€™d just make a simple video with my logo as a static image.
Then I thought â€” why not vibe code a video glitch generator?
First, to see if I can actually pull it off, and second, to see if I like the result.
To be honest, I didnâ€™t have high expectations.

I started with a very basic prompt (unfortunately I didnâ€™t save them):

> I want a glitch video generator based on an input audio file.

The AI generated some Python code, told me what to install, and how to run the script.
One package was missing, but it was easy to figure out from the error message.
After a few retries and a couple of error exchanges with the AI, it finally worked.

The first successful output surprised me â€” I actually liked it a lot!
But there was room for improvement. The video looked good but didnâ€™t really react to the audio.
So I prompted the AI again â€” asking it to analyze the trackâ€™s frequencies, create frequency groups, and apply visual variations based on dB levels.
I also told it to use full CPU power for faster testing.

The result was better â€” the visuals started responding to the music more.

But it still wasnâ€™t enough. The generator couldnâ€™t really understand the context of the track â€” the intro, main part, outro.
And I wanted more control over that.
So I asked for a way to feed it a YAML configuration â€” with both global and time-based settings â€”
so I could tailor the final output exactly how I wanted.
I also asked it to generate documentation and an example for the config file.

The AI made the changes, gave me the docs, and it worked perfectly.
Finally, I described my track sections to the AI â€” what happens in each part â€”
and it generated the final YAML config that shaped the video exactly as I imagined.

Thatâ€™s it. It took me around 3 hours total, including running the script and reviewing each video output.

Iâ€™m sharing this because I was really surprised I got this result in such a short time â€”
and I wanted to share my chain of thought behind this whole vibe coding process.

I hope you enjoy the post â€” and also the music and the final video result! ðŸŽ¶

You can find the code and watch the video in the link below ðŸ‘‡

[https://www.youtube.com/watch?v=cQM2q7FEMQI](https://www.youtube.com/watch?v=cQM2q7FEMQI)
[https://github.com/mikezaby/glitch-generator](https://github.com/mikezaby/glitch-generator)
